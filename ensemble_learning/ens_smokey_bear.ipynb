{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8da7dadf-152d-41f6-b514-b800249a9afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/nelly/.local/share/virtualenvs/ML-AI-Projects-6SkeRc7n/lib/python3.8/site-packages (3.6.0)\n",
      "Requirement already satisfied: numpy in /home/nelly/.local/share/virtualenvs/ML-AI-Projects-6SkeRc7n/lib/python3.8/site-packages (1.23.3)\n",
      "Requirement already satisfied: pandas in /home/nelly/.local/share/virtualenvs/ML-AI-Projects-6SkeRc7n/lib/python3.8/site-packages (1.5.0)\n",
      "Requirement already satisfied: scikit-learn in /home/nelly/.local/share/virtualenvs/ML-AI-Projects-6SkeRc7n/lib/python3.8/site-packages (1.1.2)\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-1.6.2-py3-none-manylinux2014_x86_64.whl (255.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 255.9 MB 120 kB/s  eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: fonttools>=4.22.0 in /home/nelly/.local/share/virtualenvs/ML-AI-Projects-6SkeRc7n/lib/python3.8/site-packages (from matplotlib) (4.37.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/nelly/.local/share/virtualenvs/ML-AI-Projects-6SkeRc7n/lib/python3.8/site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/nelly/.local/share/virtualenvs/ML-AI-Projects-6SkeRc7n/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/nelly/.local/share/virtualenvs/ML-AI-Projects-6SkeRc7n/lib/python3.8/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/nelly/.local/share/virtualenvs/ML-AI-Projects-6SkeRc7n/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nelly/.local/share/virtualenvs/ML-AI-Projects-6SkeRc7n/lib/python3.8/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/nelly/.local/share/virtualenvs/ML-AI-Projects-6SkeRc7n/lib/python3.8/site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/nelly/.local/share/virtualenvs/ML-AI-Projects-6SkeRc7n/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/nelly/.local/share/virtualenvs/ML-AI-Projects-6SkeRc7n/lib/python3.8/site-packages (from pandas) (2022.2.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/nelly/.local/share/virtualenvs/ML-AI-Projects-6SkeRc7n/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/nelly/.local/share/virtualenvs/ML-AI-Projects-6SkeRc7n/lib/python3.8/site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/nelly/.local/share/virtualenvs/ML-AI-Projects-6SkeRc7n/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/nelly/.local/share/virtualenvs/ML-AI-Projects-6SkeRc7n/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.6.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib numpy pandas scikit-learn xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ac8dd0f-f054-49a6-9f87-25381951507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df51832c-ab55-4eb9-87fe-c05c4ac63389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of data:  (62630, 16)\n",
      "44757.0 Positive cases\n",
      "17873.0 Negative cases\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>UTC</th>\n",
       "      <th>Temperature[C]</th>\n",
       "      <th>Humidity[%]</th>\n",
       "      <th>TVOC[ppb]</th>\n",
       "      <th>eCO2[ppm]</th>\n",
       "      <th>Raw H2</th>\n",
       "      <th>Raw Ethanol</th>\n",
       "      <th>Pressure[hPa]</th>\n",
       "      <th>PM1.0</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>NC0.5</th>\n",
       "      <th>NC1.0</th>\n",
       "      <th>NC2.5</th>\n",
       "      <th>CNT</th>\n",
       "      <th>Fire Alarm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1654733331</td>\n",
       "      <td>20.000</td>\n",
       "      <td>57.36</td>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>12306</td>\n",
       "      <td>18520</td>\n",
       "      <td>939.735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1654733332</td>\n",
       "      <td>20.015</td>\n",
       "      <td>56.67</td>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>12345</td>\n",
       "      <td>18651</td>\n",
       "      <td>939.744</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1654733333</td>\n",
       "      <td>20.029</td>\n",
       "      <td>55.96</td>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>12374</td>\n",
       "      <td>18764</td>\n",
       "      <td>939.738</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1654733334</td>\n",
       "      <td>20.044</td>\n",
       "      <td>55.28</td>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>12390</td>\n",
       "      <td>18849</td>\n",
       "      <td>939.736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1654733335</td>\n",
       "      <td>20.059</td>\n",
       "      <td>54.69</td>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>12403</td>\n",
       "      <td>18921</td>\n",
       "      <td>939.744</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         UTC  Temperature[C]  Humidity[%]  TVOC[ppb]  eCO2[ppm]  \\\n",
       "0           0  1654733331          20.000        57.36          0        400   \n",
       "1           1  1654733332          20.015        56.67          0        400   \n",
       "2           2  1654733333          20.029        55.96          0        400   \n",
       "3           3  1654733334          20.044        55.28          0        400   \n",
       "4           4  1654733335          20.059        54.69          0        400   \n",
       "\n",
       "   Raw H2  Raw Ethanol  Pressure[hPa]  PM1.0  PM2.5  NC0.5  NC1.0  NC2.5  CNT  \\\n",
       "0   12306        18520        939.735    0.0    0.0    0.0    0.0    0.0    0   \n",
       "1   12345        18651        939.744    0.0    0.0    0.0    0.0    0.0    1   \n",
       "2   12374        18764        939.738    0.0    0.0    0.0    0.0    0.0    2   \n",
       "3   12390        18849        939.736    0.0    0.0    0.0    0.0    0.0    3   \n",
       "4   12403        18921        939.744    0.0    0.0    0.0    0.0    0.0    4   \n",
       "\n",
       "   Fire Alarm  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"smoke_detection_iot.csv\"\n",
    "sd_data = pd.read_csv(filename)\n",
    "sd_vector = sd_data.to_numpy()\n",
    "print(\"Size of data: \",sd_vector.shape)\n",
    "print(f\"{np.sum(sd_vector[:,-1])} Positive cases\")\n",
    "print(f\"{sd_vector.shape[0] - np.sum(sd_vector[:,-1])} Negative cases\")\n",
    "sd_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fa2514-2b9b-419f-8a21-b7e7aa16dc9c",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Decision Tree Classifier Implementation. Solving the problem from last assignment of fire detection with sensor fusion.\n",
    "\n",
    "First starting with the vanilla implementation and testing its accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "842a4590-a305-4e5b-8502-6026baddc24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 100.0000%\n",
      "Testing Accuracy: 43.2780%\n"
     ]
    }
   ],
   "source": [
    "def preprocess(sd_vector):\n",
    "    # 80% train 20% test split\n",
    "    train_size = int(sd_vector.shape[0]*0.8)\n",
    "    x = sd_vector[:,2:-2] \n",
    "    # normalize data\n",
    "    x = (x-np.mean(x,axis=0))/(np.std(x,axis=0))\n",
    "    y = sd_vector[:,-1]\n",
    "    return x[:train_size].copy(),y[:train_size].copy(),x[train_size:].copy(),y[train_size:].copy()\n",
    "\n",
    "x_train, y_train, x_test, y_test = preprocess(sd_vector)\n",
    "\n",
    "# Running the default model\n",
    "rfc = DecisionTreeClassifier()\n",
    "rfc.fit(x_train,y_train)\n",
    "y_pred = rfc.predict(x_train)\n",
    "print(f\"Training Accuracy: {100*metrics.accuracy_score(y_train, y_pred):0.4f}%\")\n",
    "y_pred = rfc.predict(x_test)\n",
    "print(f\"Testing Accuracy: {100*metrics.accuracy_score(y_test, y_pred):0.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d7e019-b35a-450c-a876-16d581f67be2",
   "metadata": {},
   "source": [
    "As we can see the decision tree from scikit learn with default parameters slightly underperforms the linear regressor from the last assignment that reached a near 70% accuracy. \n",
    "\n",
    "Next trying to overcome the data imbalances in the last assignment by setting the decision tree to balance the dataset weighting before training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b576f13-a817-4892-9272-0fb7fbd4d49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 100.0000%\n",
      "Testing Accuracy: 83.4265%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rfc = DecisionTreeClassifier(class_weight =\"balanced\")\n",
    "rfc.fit(x_train,y_train)\n",
    "y_pred = rfc.predict(x_train)\n",
    "print(f\"Training Accuracy: {100*metrics.accuracy_score(y_train, y_pred):0.4f}%\")\n",
    "y_pred = rfc.predict(x_test)\n",
    "print(f\"Testing Accuracy: {100*metrics.accuracy_score(y_test, y_pred):0.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a01c24e-260d-4a85-9b5e-ed006e766c1a",
   "metadata": {},
   "source": [
    "From this test we can see a balanced dataset for the decision tree drastically improves performance to a higher score then acheived with linear classifiers. This so far is the best performing method for smoke detection.\n",
    "\n",
    "Next changing the model from selecting the best criteria in every node of the tree to selecting a crieteria randomly to hopefully get better generalization without the greedy approach. Because this changes the model training dynamics to be highly variable I will run this experiment multiple times to see the best model it produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39b81a3b-44c5-453d-8352-6999254532b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Training Accuracy: 100.0000%\n",
      "Model 1 Testing Accuracy: 83.4265%\n",
      "Model 2 Training Accuracy: 100.0000%\n",
      "Model 2 Testing Accuracy: 83.4265%\n",
      "Model 3 Training Accuracy: 100.0000%\n",
      "Model 3 Testing Accuracy: 83.4265%\n",
      "Model 4 Training Accuracy: 100.0000%\n",
      "Model 4 Testing Accuracy: 83.4265%\n",
      "Model 5 Training Accuracy: 100.0000%\n",
      "Model 5 Testing Accuracy: 83.4265%\n",
      "Model 6 Training Accuracy: 100.0000%\n",
      "Model 6 Testing Accuracy: 83.4265%\n",
      "Model 7 Training Accuracy: 100.0000%\n",
      "Model 7 Testing Accuracy: 83.4265%\n",
      "Model 8 Training Accuracy: 100.0000%\n",
      "Model 8 Testing Accuracy: 83.4265%\n",
      "Model 9 Training Accuracy: 100.0000%\n",
      "Model 9 Testing Accuracy: 83.4265%\n",
      "Model 10 Training Accuracy: 100.0000%\n",
      "Model 10 Testing Accuracy: 83.4265%\n",
      "Model 11 Training Accuracy: 100.0000%\n",
      "Model 11 Testing Accuracy: 83.4265%\n",
      "Model 12 Training Accuracy: 100.0000%\n",
      "Model 12 Testing Accuracy: 83.4265%\n",
      "Model 13 Training Accuracy: 100.0000%\n",
      "Model 13 Testing Accuracy: 83.4265%\n",
      "Model 14 Training Accuracy: 100.0000%\n",
      "Model 14 Testing Accuracy: 83.4265%\n",
      "Model 15 Training Accuracy: 100.0000%\n",
      "Model 15 Testing Accuracy: 83.4265%\n",
      "Model 16 Training Accuracy: 100.0000%\n",
      "Model 16 Testing Accuracy: 83.4265%\n",
      "Model 17 Training Accuracy: 100.0000%\n",
      "Model 17 Testing Accuracy: 83.4265%\n",
      "Model 18 Training Accuracy: 100.0000%\n",
      "Model 18 Testing Accuracy: 83.4265%\n",
      "Model 19 Training Accuracy: 100.0000%\n",
      "Model 19 Testing Accuracy: 83.4265%\n",
      "Model 20 Training Accuracy: 100.0000%\n",
      "Model 20 Testing Accuracy: 83.4265%\n"
     ]
    }
   ],
   "source": [
    "# Loop to train multiple random models\n",
    "for i in range(20):\n",
    "    rfco = DecisionTreeClassifier(class_weight =\"balanced\", splitter =\"random\")\n",
    "    rfco.fit(x_train,y_train)\n",
    "    y_pred = rfco.predict(x_train)\n",
    "    print(f\"Model {i+1} Training Accuracy: {100*metrics.accuracy_score(y_train, y_pred):0.4f}%\")\n",
    "    y_pred = rfc.predict(x_test)\n",
    "    print(f\"Model {i+1} Testing Accuracy: {100*metrics.accuracy_score(y_test, y_pred):0.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13eff48-4d22-41ea-bd77-c9971b84c9fb",
   "metadata": {},
   "source": [
    "From this experiment we can see that while random decisions make the model on average perform less accuratly, by searching the model space we can actually find models that perform near 90% accuracy on the training and test sets. It is worth to note though with this sort of bias towards selecting higher performing models on the test set we may be biasing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292c62bb-d48a-432d-8bf1-1cd2f85a4f96",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "From the Bagging and Boosting ensemble methods pick any one algorithm\n",
    "from each category. Implement both the algorithms using the same data. Use k-fold cross\n",
    "validation to find the effectiveness of both the models. \n",
    "\n",
    "\n",
    "Starting with Bagging using KNeighborsClassifier implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f6e4694-9fb7-4b40-ad8b-cae8302888b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99872266 0.99792432 0.99936133 0.99920166 0.99776465 0.99776465\n",
      " 0.99840332 0.99888232 0.99856299 0.99888232 0.99840332 0.99776465\n",
      " 0.99808399 0.99936133 0.99904199 0.99840332 0.99856299 0.99776465\n",
      " 0.99888232 0.99872266 0.99872266 0.99888232 0.99776465 0.99840332\n",
      " 0.99872266 0.99840332 0.99824365 0.99904199 0.999521   0.99744531]\n",
      "Accuracy: 0.999 (0.001)\n",
      "Done\n",
      "Model Testing Accuracy: 14.9848%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "x_train, y_train, x_test, y_test = preprocess(sd_vector)\n",
    "\n",
    "X  = np.concatenate([x_train,x_test],axis=0)\n",
    "Y  = np.concatenate([y_train,y_test],axis=0)\n",
    "\n",
    "# Support vector machine bagging clasisfier\n",
    "bag_model = BaggingClassifier(base_estimator=KNeighborsClassifier(),n_estimators =5 )\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(bag_model, X, Y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print(n_scores)\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "print(\"Done\")\n",
    "\n",
    "bag_model.fit(x_train,y_train)\n",
    "y_pred = bag_model.predict(x_test)\n",
    "print(f\"Model Testing Accuracy: {100*metrics.accuracy_score(y_test, y_pred):0.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2274ca0e-78e2-4b26-b967-4688e7a006e1",
   "metadata": {},
   "source": [
    "These are the results of the KNeighbors based classifier.\n",
    "From this we can see the k-fold cross validation with 10 splits resulted in an extremely high model accuracy of 0.999 average across all of the folds with a low standard deviation of 0.001. However, when actually fit on a dataset the examples went down to 15% accuracy. This could be due to the different fitting metrics the scikitlearn cross validation uses or simply the differences in having a relatively high train test ratio vs a low ratio in the low accuracy example.\n",
    "\n",
    "Next implementing the Boosting method with the XGBoost implementation that includes regularization to help with overfitting while improving the performance of stock gradient boosting algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "356a08fa-d4ea-43d1-9825-80ca1026d2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99936133 0.99984033 1.         0.99984033 0.99984033 0.99920166\n",
      " 0.99968066 0.999521   0.99968066 0.99920166 0.999521   0.99968066\n",
      " 0.99984033 0.999521   0.99984033 0.999521   0.99936133 0.99984033\n",
      " 0.99968066 1.         0.99936133 0.99984033 0.99968066 0.99984033\n",
      " 0.999521   0.999521   0.99984033 1.         0.99968066 0.999521  ]\n",
      "Accuracy: 1.000 (0.000)\n",
      "Done\n",
      "Model Testing Accuracy: 56.8338%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "x_train, y_train, x_test, y_test = preprocess(sd_vector)\n",
    "\n",
    "X  = np.concatenate([x_train,x_test],axis=0)\n",
    "Y  = np.concatenate([y_train,y_test],axis=0)\n",
    "\n",
    "boost_model = xgb.XGBClassifier(n_estimators = 5)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(boost_model, X, Y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print(n_scores)\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "print(\"Done\")\n",
    "\n",
    "boost_model.fit(x_train, y_train)\n",
    "y_pred = boost_model.predict(x_test)\n",
    "print(f\"Model Testing Accuracy: {100*metrics.accuracy_score(y_test, y_pred):0.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d16d1a6-21c6-4f82-8a96-becaf3c9031d",
   "metadata": {},
   "source": [
    "The boosting method seems to have performed very similarly to the bagging model with slightly higher cross validation scores. For differences the extreme gradient boosting method seems to be an order of magnitude faster then the bagging method likely due to the fact that the boosting method can be run in parrallel and the individual model sizes for this model are much much smaller. Along with this, the training and testing figures after the cross validation indicate the boosting method is more stable as it performed better in the one off training example while the bagging method had failed. Both models seemed to struggle with the particular training test set I used for my training and evaluation while both models did well with the cross validation. This may be due to improper class balancing as my test and training sets were selected sequentially potentially leading to harder examples in the relatively large test set that the model struggles with unlike the smaller partitions in cross validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667343ee-172a-4703-adb4-6022170cfcbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 3 \n",
    "Comparing the three models implemented above utilizing the various stats that can be derived from a confusion matrix as they help contextualize a model's base accuracy and indicate exactly how the model will perform in the real world. \n",
    "\n",
    "Confusion matrixes show the difference between true and false positive and negative predictions to hopefully reveal and mitigate model bias. In this case of fire detections an extremely high precision is required since missing a fire would be devastating but also triggering false alarms will desensitize people to the fire alarm. Along with this recall from the confusion matrix can be used to see just how many actual cases were missed and how many were correctly classified. Also the imbalance in classes for this binary classifcation task lends itself to model bias. Simply looking at accuracy does not reveal the depth of information needed to decide if a model should be deployed in the real world.\n",
    "\n",
    "This source gives a great example of where confusion matrices are needed for model analysis (https://machinelearningmastery.com/confusion-matrix-machine-learning/#:~:text=Classification%20accuracy%20alone%20can%20be,of%20errors%20it%20is%20making.) and was part of the motivation of me using this technique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78b88311-7f36-4581-a80b-fc2747fa083f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Model\n",
      "Confusion Matrix:  [[10324  1160]\n",
      " [  916   126]]\n",
      "Precision 89.90%\n",
      "Accuracy 83.43%\n",
      "Recall  91.85%\n",
      "Specificity  9.80%\n",
      "\n",
      "Second Model\n",
      "Confusion Matrix:  [[1588 9896]\n",
      " [ 753  289]]\n",
      "Precision 13.83%\n",
      "Accuracy 14.98%\n",
      "Recall  67.83%\n",
      "Specificity  2.84%\n",
      "\n",
      "Third Model\n",
      "Confusion Matrix:  [[7016 4468]\n",
      " [ 939  103]]\n",
      "Precision 61.09%\n",
      "Accuracy 56.83%\n",
      "Recall  88.20%\n",
      "Specificity  2.25%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "x_train, y_train, x_test, y_test = preprocess(sd_vector)\n",
    "\n",
    "# second model from task 1\n",
    "print(\"First Model\")\n",
    "y_pred = rfc.predict(x_test)\n",
    "one_cof = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix: \",one_cof)\n",
    "print(f\"Precision {100*(one_cof[0][0]/(one_cof[0][0]+one_cof[0][1])):0.2f}%\")\n",
    "print(f\"Accuracy {100*((one_cof[0][0]+one_cof[1][1])/(np.sum(one_cof))):0.2f}%\")\n",
    "print(f\"Recall  {100*(one_cof[0][0]/(one_cof[0][0]+one_cof[1][0])):0.2f}%\")\n",
    "print(f\"Specificity  {100*(one_cof[1][1]/(one_cof[1][1]+one_cof[0][1])):0.2f}%\")\n",
    "print()\n",
    "\n",
    "# bagging model\n",
    "print(\"Second Model\")\n",
    "y_pred = bag_model.predict(x_test)\n",
    "two_cof = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix: \",two_cof)\n",
    "print(f\"Precision {100*(two_cof[0][0]/(two_cof[0][0]+two_cof[0][1])):0.2f}%\")\n",
    "print(f\"Accuracy {100*((two_cof[0][0]+two_cof[1][1])/(np.sum(two_cof))):0.2f}%\")\n",
    "print(f\"Recall  {100*(two_cof[0][0]/(two_cof[0][0]+two_cof[1][0])):0.2f}%\")\n",
    "print(f\"Specificity  {100*(two_cof[1][1]/(two_cof[1][1]+two_cof[0][1])):0.2f}%\")\n",
    "print()\n",
    "\n",
    "# boosting model\n",
    "print(\"Third Model\")\n",
    "y_pred = boost_model.predict(x_test)\n",
    "three_cof = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix: \",three_cof)\n",
    "print(f\"Precision {100*(three_cof[0][0]/(three_cof[0][0]+three_cof[0][1])):0.2f}%\")\n",
    "print(f\"Accuracy {100*((three_cof[0][0]+three_cof[1][1])/(np.sum(three_cof))):0.2f}%\")\n",
    "print(f\"Recall  {100*(three_cof[0][0]/(three_cof[0][0]+three_cof[1][0])):0.2f}%\")\n",
    "print(f\"Specificity  {100*(three_cof[1][1]/(three_cof[1][1]+three_cof[0][1])):0.2f}%\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181772ed-2012-49af-8283-623d0b458977",
   "metadata": {},
   "source": [
    "By analyzing these various metrics we can see the pro and con of each model and what information would be lost by only analyzing accuracy. The first destinction between the models is that across the board the First model with data balancing random forest outperforms all other methods of ensamble learning. This is likely due to the data balancing as all other models have to compensate for the data imbalance. The recall and precision of this model are high as well indicating that the model accurately assigns it's positive cases which is important in fire detection to avoiding false alarms that desensitize people's reactions.\n",
    "\n",
    "An accuracy of only 80% makes the first model seem poor for fire detection but you would be missing the model's impressive low true negative rate indicating that not many fires would be missed with this alarm. This is also true for the third model that has a low accuracy but a high recall so it's rate of predicted positives over the true positives is high indicating the model is good at detecting fires when present but it struggles with precision meaning it has many false positives. Not only is this useful with fine tuning the model but also shows how one metric may appear high while the model's representation of the problem is poor. This Third model likely is clearly predicting true in nearly all cases and since the dataset is biased towards present fires. The second model is relatively unremarkable in metric analysis since in all regards it is poor at predicting, although even the even spread of this model can indicate a training failure as in binary classification 50% accuracy should be expected in a balanced dataset.\n",
    "\n",
    "Overall, multiple metrics should always be used in evaluating different machine learning models as they can help with fine tuning their output and predicting what problems will arise in ht ereal world. The confusion matrix is great for this role as it offers multiple perspectives of a simple classification problem and is probably why this practice is so common in the feild. In the future I hope to use confusion matrices to analyze the performance of classification problems especially in spaces that are sensitive to false positives such as fire detection, cancer detection, and model's whose positive case should be relatively rare in the real world.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ae093-c441-442f-a776-ed15c146b427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
